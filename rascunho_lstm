


    model.add(layers.Conv1D(12, kernel_size=2, padding='same', activation='relu',
                    kernel_regularizer='l2',
                    kernel_initializer = "lecun_normal" ))
    model.add(layers.Conv1D(12, kernel_size=1, padding='same', activation='relu',
                    kernel_regularizer='l2',
                    kernel_initializer = "lecun_normal" ))
    model.add(layers.MaxPooling1D(pool_size=3))
    model.add(layers.BatchNormalization())
    model.add(layers.Flatten())
    #model.add(
    #layers.Bidirectional(
    #    layers.LSTM(50, return_sequences=True, 
    #                input_shape=(train_X.shape[1], train_X.shape[2]),
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu')))
    #model.add(
    #layers.Bidirectional(
    #    layers.LSTM(50, 
    #                input_shape=(train_X.shape[1], train_X.shape[2]),
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu')))
    ##model.add(layers.Conv1D(12, kernel_size=9, padding='same', activation='relu'))
    #model.add(layers.Flatten())
    #model.add(layers.Dropout(0.2))
    #model.add(tfa.layers.WeightNormalization(
    #    layers.Dense(32,
    #                 kernel_regularizer='l2',
    #                 kernel_initializer = "lecun_normal",
    #                 activation='relu')
    #))
    #model.add(layers.Dropout(0.2))
    #model.add(tfa.layers.WeightNormalization(
    #    layers.Dense(32,
    #                 kernel_regularizer='l2',
    #                 kernel_initializer = "lecun_normal",
    #                 activation='relu')
    #))
    
    
    
    
    #model.add(
    #layers.Bidirectional(
    #    layers.LSTM(50, return_sequences=True, 
    #                input_shape=(train_X.shape[1], train_X.shape[2]),
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu')))
    #model.add(layers.Bidirectional(
    #    layers.LSTM(50, #return_sequences=True,
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu')))
    #model.add(layers.RepeatVector(3))
    #model.add(layers.Flatten())
    #model.add(tfa.layers.WeightNormalization(
    #    layers.Dense(64,
    #                 kernel_regularizer='l2',
    #                 kernel_initializer = "lecun_normal",
    #                 activation='relu')
    #))
    
    #model.add(layers.LSTM(16, return_sequences=True, 
    #                input_shape=(train_X.shape[1], train_X.shape[2]),
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu'))
    #model.add(layers.LSTM(16, #return_sequences=True, 
    #                input_shape=(train_X.shape[1], train_X.shape[2]),
    #                kernel_regularizer='l2',
    #                kernel_initializer = "lecun_normal",
    #                activation='relu'))